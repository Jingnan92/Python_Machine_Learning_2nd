{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) : a lazy learner algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN is a lazy learner: it doesn't learn a discriminative function from the training data, but memorizes the training dataset instead.\n",
    "- Nonparametric model: it can't be characterized by a fixed set of parameters, and the number of parameters grows with the training data. E.g. decision tree, random forest, and kernel SVM\n",
    "- Instance-based learning: it memorizes the training dataset and there is no cost during the leraning process.\n",
    "- Steps:\n",
    "     1. Choose the number of k and a distance metric\n",
    "     2. Find the k-nearest neighbors of the sample that we want to classify\n",
    "     3. Assign the class label by majority vote\n",
    "- Pros and Cons:\n",
    "     1. Pros: adapts immediately as we colect new training data\n",
    "     2. Cons: computational complexity grwos linearly with the number of samples in the training set, works poorly with high-dimensional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN in Sklearn\n",
    "\n",
    "sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
    "\n",
    "Parameters:\n",
    "- n_neighborsint: Number of neighbors to use by default for kneighbors queries, default=5\n",
    "- weights: weight function used in prediction, {‘uniform’, ‘distance’}, default=’uniform’\n",
    "- algorithm: Algorithm used to compute the nearest neighbors, {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "- leaf_size: Leaf size passed to BallTree or KDTree, default = 30\n",
    "- p: Power parameter for  Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used, default = 2\n",
    "- metrics: the distance metric to use for the tree, {'euclidean', 'manhattan', 'minkowski'}, default=’minkowski’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric = 'minkowski' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selections:\n",
    "\n",
    "- K: balance between overfitting and underfitting\n",
    "- distance metric: \n",
    "\n",
    "    $d(x^{(i)}, x^{(j)}) = p \\sqrt{\\sum_k|x^{(i)} - x^{(j)}|^p}$\n",
    "    \n",
    "    p=1 for Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
