{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data\n",
    "\n",
    "- Identifying missing values\n",
    "- Technique to address missing value issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identifying missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the values in dataframe/column\n",
    "df.values\n",
    "#  count the number of missing values per column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing missing values\n",
    "Note: potential loss of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop both columns and rows with missing values\n",
    "df.dropna(how = 'all')\n",
    "\n",
    "# drop rows/instances with missing values\n",
    "df.dropna(axis = 0)\n",
    "\n",
    "#drop columns/features with missing values\n",
    "df.dropna(axis = 1)\n",
    "\n",
    "# drop instances with more than k NAs\n",
    "df.dropna(thresh = k)\n",
    "\n",
    "# drop rows where NaN appear in specific columns\n",
    "df.dropna(subset = ['column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values\n",
    "- replace the missing value with the mean/median/most freqent (for categorical variable) value of the entire feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imr = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0)\n",
    "# strategy: mean or median for continuous values, most_frequent for categorical variable\n",
    "# axis = 0 for column-wise imputation, axis = 1 for row-wise imputation\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical variable\n",
    "\n",
    "- Ordinal features\n",
    "- Nominal features\n",
    "- Encoding class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal features: categorical variablesthat can be sorted or ordered\n",
    "\n",
    "     mapping ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dic = {'category': 'value'}\n",
    "df['ordinal'] = df['ordinal'].map(map_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal featuers: doesn't imply any order\n",
    "    one-hot encoding on nominal features: to create a new dummy feature for each unique value in the nominal feature column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method A\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categories = [i], sparse = True/False)\n",
    "ohe.fit_transform(X).toarray()\n",
    "\n",
    "# Method B: \n",
    "pd.get_dummies(df[['nominal_features']])\n",
    "# potential problem of multicollinearity, remove one feature column\n",
    "pd.get_dummies(df[['nominal_features']], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding class labels:  class labels as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "- Feature scaling is crucial for distance/similarity based algorithm, such as SVM, KNN, K-means, etc. Decision trees and random forests are two of the very few machine learning algorithms that are scale invariant.\n",
    "\n",
    "- Two common approaches for feature scaling:\n",
    "    1. Normalization: rescale the features to a range of [0,1], a special case of min-max scaling\n",
    "        \n",
    "        $x_{norm}^{(i)} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$\n",
    "        \n",
    "    2. Standardization: center the feature columns at mean 0 with standard deviation 1, which makes it easier to leran the weights for gradient descent algorithms. This also maintains useful iinformation about the outliers and makes the algorithm less sensitive to them in contrast to min-max scaling.\n",
    "    \n",
    "        $x_{std}^{(i)} = \\frac{x^{(i)}-\\mu_x}{\\sigma_x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "\n",
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)\n",
    "\n",
    "# Note: fit scaler only once on the training data and use those parameters to transform the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common solutions to reduce the generalization errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "- L1 and L2 regularization\n",
    "- adding bias and preferring a simpler model to reduce the variance in the absence of sufficient training data to fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "- Feature selection:\n",
    "    1. Sequential feature selection algorithms: a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k<d. The motivation is to automatically select a subset of features that are most relevant to the problem, to improve the computational efficiency or reduce the generalization error of the model by removing irrelevant featurs or noise.\n",
    "    2. Tree-based models to select features by importance\n",
    "    3. Univariate statistical tests\n",
    "    \n",
    "- Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
